{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisa11323/CSR_Laddit/blob/main/Study_1_empath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Preprocessed data"
      ],
      "metadata": {
        "id": "gmDuakhNBuh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove non-English characters + strip emojis + filter by syllable count"
      ],
      "metadata": {
        "id": "Toh7dJxLCTJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pandas numpy\n",
        "!pip install nltk emoji"
      ],
      "metadata": {
        "id": "NhkamJh_CH2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ğŸ“ íŒŒì¼ ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# â³ tqdmê³¼ pandas ì—°ë™\n",
        "tqdm.pandas()\n",
        "\n",
        "# ğŸ“„ ì—‘ì…€ íŒŒì¼ ì½ê¸°\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "# ğŸ§¹ ì´ëª¨ì§€ + ë¹„ì˜ì–´ ë¬¸ì ì œê±° í•¨ìˆ˜\n",
        "def clean_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    text = emoji.replace_emoji(text, replace=\"\")\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# ğŸ”„ ì „ì²˜ë¦¬ ì ìš© (ê¸°ì¡´ 'Review' ë®ì–´ì“°ê¸°)\n",
        "df['Review'] = df['Review'].progress_apply(clean_text)\n",
        "\n",
        "# âœ¨ ë¦¬ë·° ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜\n",
        "def get_review_length(text):\n",
        "    try:\n",
        "        return len(str(text).split())\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# ğŸ“ Review_length ì»¬ëŸ¼ ìƒì„±\n",
        "df['Review_length'] = df['Review'].progress_apply(get_review_length)\n",
        "\n",
        "# ğŸš« ë¦¬ë·° ê¸¸ì´ 10 ë¯¸ë§Œ ì œê±°, ê¸°ì¤€ì„ ë°”ê¾¸ë ¤ë©´ ì—¬ê¸°ì— ìˆëŠ” ìˆ«ì 10ì„ ìˆ˜ì •\n",
        "df = df[df['Review_length'] >= 10].reset_index(drop=True)\n",
        "\n",
        "# âœ… ì—‘ì…€ë¡œ ì €ì¥\n",
        "preprocessed_file_path = 'preprocessed_comments_with_length.xlsx'\n",
        "df.to_excel(preprocessed_file_path, index=False)\n",
        "print(f\"âœ… ì „ì²˜ë¦¬ + ë¦¬ë·° ê¸¸ì´ â‰¥10 í•„í„°ë§ ë°ì´í„°ë¥¼ '{preprocessed_file_path}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ğŸ“¥ ë‹¤ìš´ë¡œë“œ\n",
        "files.download(preprocessed_file_path)"
      ],
      "metadata": {
        "id": "a9z1PF8JCIYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the Top-N most frequent words from the Reddit comment data"
      ],
      "metadata": {
        "id": "8-pMqIM7CLsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "print(f\"âœ… ì´ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ\")\n",
        "\n",
        "# ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°\n",
        "all_text = ' '.join(df['comment'].dropna().astype(str).tolist()).lower()\n",
        "\n",
        "# ë‹¨ì–´ë§Œ ì¶”ì¶œ\n",
        "words = re.findall(r'\\b\\w+\\b', all_text)\n",
        "\n",
        "# ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# ìƒìœ„ 50ê°œ ë‹¨ì–´ ì¶œë ¥\n",
        "top_n = 50\n",
        "print(\"\\nğŸ”· ìƒìœ„ ë‹¨ì–´ Top-50:\")\n",
        "for word, count in word_counts.most_common(top_n):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "t8cWv_GsBuKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¶ˆìš©ì–´ ì œê±°\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [w for w in words if w not in stop_words]\n",
        "\n",
        "# ë¶ˆìš©ì–´ ì œê±° í›„ ë‹¨ì–´ ë¹ˆë„ ì¬ê³„ì‚°\n",
        "filtered_word_counts = Counter(filtered_words)\n",
        "\n",
        "print(\"\\nğŸ”· ë¶ˆìš©ì–´ ì œê±° í›„ ìƒìœ„ ë‹¨ì–´ Top-50:\")\n",
        "for word, count in filtered_word_counts.most_common(50):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "gE6WJn5hB-r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Independent Variable"
      ],
      "metadata": {
        "id": "xY9d4X1WAtG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trust"
      ],
      "metadata": {
        "id": "TEt8K9yvA-XP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClxZ5pV4Ab0z"
      },
      "outputs": [],
      "source": [
        "# ğŸ“¦ Install libraries\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# ğŸ“š Import libraries\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# â¬†ï¸ Upload data\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ Load data\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# âœ… Check required column\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ The 'comment' column is missing from the data!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"âŒ The 'comment' column contains no data to analyze!\")\n",
        "\n",
        "print(f\"âœ… Total number of comments: {len(df)}\")\n",
        "\n",
        "# ğŸ¤– Load Empath\n",
        "lexicon = Empath()\n",
        "print(\"ğŸ”· The default 'trust' category in Empath is handled internally.\")\n",
        "\n",
        "# ğŸ”· Custom Reddit-specific trust word list\n",
        "reddit_trust_words_full = [\n",
        "    'vouch', 'got', 'back', 'solid', 'faithful', 'reliable', 'loyal',\n",
        "    'stand', 'based', 'legit', 'support', 'believe', 'integrity',\n",
        "    'ride', 'crew', 'dayone', 'fam', 'homie', 'kin',\n",
        "    'respect', 'props', 'cover', 'cosign', 'trustworthy', 'dependable',\n",
        "    'buddy', 'partner', 'ally', 'friend', 'assist', 'helpful'\n",
        "]\n",
        "\n",
        "# âœ… Remove duplicates & sort\n",
        "reddit_trust_words_unique = sorted(set(reddit_trust_words_full))\n",
        "print(f\"ğŸ”· Number of unique Reddit-specific trust words: {len(reddit_trust_words_unique)}\")\n",
        "print(reddit_trust_words_unique)\n",
        "\n",
        "# ğŸ”· Create custom category\n",
        "lexicon.create_category(\"reddit_trust\", reddit_trust_words_unique)\n",
        "print(\"âœ… Custom category 'reddit_trust' created successfully!\")\n",
        "\n",
        "# ğŸ”· Verify that categories are properly registered (optional)\n",
        "test_result = lexicon.analyze(\"this is a test\", categories=[\"trust\", \"reddit_trust\"])\n",
        "if not all(cat in test_result for cat in [\"trust\", \"reddit_trust\"]):\n",
        "    raise RuntimeError(\"âŒ Either 'trust' or 'reddit_trust' category was not registered correctly!\")\n",
        "\n",
        "# ğŸ§  Analysis function: default trust + custom reddit_trust + combined\n",
        "def analyze_and_combine_trust(text):\n",
        "    result = lexicon.analyze(str(text), categories=[\"trust\", \"reddit_trust\"], normalize=True)\n",
        "    trust_score = result.get('trust', 0.0)\n",
        "    reddit_trust_score = result.get('reddit_trust', 0.0)\n",
        "    combined_trust = trust_score + reddit_trust_score\n",
        "    return pd.Series({\n",
        "        \"trust_score\": round(trust_score, 4),\n",
        "        \"reddit_trust_score\": round(reddit_trust_score, 4),\n",
        "        \"combined_trust_score\": round(combined_trust, 4)\n",
        "    })\n",
        "\n",
        "# ğŸƒ Run analysis\n",
        "tqdm.pandas()\n",
        "trust_df = df['comment'].progress_apply(analyze_and_combine_trust)\n",
        "\n",
        "# ğŸ”— Combine results\n",
        "result_df = pd.concat([df, trust_df], axis=1)\n",
        "\n",
        "# ğŸ’¾ Save results\n",
        "output_file = filename.replace(\".xlsx\", \"_trust_combined_empath_expanded.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"âœ… Done: Metrics including the extended custom trust scores have been calculated and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "social engagement"
      ],
      "metadata": {
        "id": "VXIDm3IwBUyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# â¬†ï¸ ë°ì´í„° ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "print(f\"âœ… ì´ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ\")\n",
        "\n",
        "# ğŸ¤– Empath ë¡œë“œ\n",
        "lexicon = Empath()\n",
        "\n",
        "# ğŸ”· ê¸°ì¡´ Social Engagement ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ ì§‘í•© ê°€ì ¸ì˜¤ê¸°\n",
        "base_words = set(lexicon.cats['communication']) \\\n",
        "           | set(lexicon.cats['meeting']) \\\n",
        "           | set(lexicon.cats['social_media'])\n",
        "\n",
        "print(f\"âœ… ê¸°ì¡´ Social Engagement ë‹¨ì–´ ìˆ˜: {len(base_words)}\")\n",
        "\n",
        "# ğŸ”· Reddit íŠ¹í™” Social Engagement ë‹¨ì–´ ë¦¬ìŠ¤\n",
        "reddit_social_engagement_words_full = [\n",
        "    'subreddit', 'thread', 'reply', 'post', 'topic', 'comment', 'dm',\n",
        "    'debate', 'discussion', 'poll', 'vote', 'feedback', 'engage',\n",
        "    'interact', 'participate',\n",
        "    'connect', 'bond', 'exchange', 'help', 'support',\n",
        "    'mention', 'tag', 'like', 'share', 'follow', 'trend'\n",
        "]\n",
        "\n",
        "# ğŸ”· ì¤‘ë³µ ì œê±° & ì •ë ¬\n",
        "reddit_social_engagement_words_unique = sorted({\n",
        "    word for word in reddit_social_engagement_words_full if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"âœ… ì¤‘ë³µ ì œê±° í›„ ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ ìˆ˜: {len(reddit_social_engagement_words_unique)}\")\n",
        "print(reddit_social_engagement_words_unique)\n",
        "\n",
        "# ğŸ”· ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ ìƒì„±\n",
        "lexicon.create_category(\"reddit_social_engagement\", reddit_social_engagement_words_unique)\n",
        "print(\"âœ… ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ 'reddit_social_engagement' ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# ğŸ”· ì¹´í…Œê³ ë¦¬ ì •ìƒ ë“±ë¡ í…ŒìŠ¤íŠ¸\n",
        "test_result = lexicon.analyze(\"this is a test post reply engage\", categories=[\n",
        "    \"communication\", \"meeting\", \"social_media\", \"reddit_social_engagement\"\n",
        "])\n",
        "expected_categories = [\"communication\", \"meeting\", \"social_media\", \"reddit_social_engagement\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"âŒ í•˜ë‚˜ ì´ìƒì˜ ì¹´í…Œê³ ë¦¬ê°€ ì˜¬ë°”ë¥´ê²Œ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ğŸ§  ë¶„ì„ í•¨ìˆ˜: ê¸°ì¡´ + ì‚¬ìš©ì ì •ì˜ + í•©ì‚°\n",
        "def analyze_and_combine_social_engagement(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"communication\", \"meeting\", \"social_media\", \"reddit_social_engagement\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('communication', 0.0),\n",
        "        result.get('meeting', 0.0),\n",
        "        result.get('social_media', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_social_engagement', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"social_engagement_base_score\": round(base_score, 4),\n",
        "        \"reddit_social_engagement_score\": round(reddit_score, 4),\n",
        "        \"combined_social_engagement_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# ğŸƒ ë¶„ì„ ì‹¤í–‰\n",
        "tqdm.pandas()\n",
        "engagement_df = df['comment'].progress_apply(analyze_and_combine_social_engagement)\n",
        "\n",
        "# ğŸ”— ë°ì´í„° í•©ì¹˜ê¸°\n",
        "result_df = pd.concat([df, engagement_df], axis=1)\n",
        "\n",
        "# ğŸ’¾ ê²°ê³¼ ì €ì¥\n",
        "output_file = filename.replace(\".xlsx\", \"_social_engagement_combined_empath.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"âœ… ì™„ë£Œ: ê¸°ì¡´ + ì‚¬ìš©ì ì •ì˜ Social Engagement ì ìˆ˜ê°€ ê³„ì‚°ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "qCJOr9JNBV9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Norms of reciprocity"
      ],
      "metadata": {
        "id": "m8jeb8M9BYSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# â¬†ï¸ ë°ì´í„° ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "print(f\"âœ… ì´ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ\")\n",
        "\n",
        "# ğŸ¤– Empath ë¡œë“œ\n",
        "lexicon = Empath()\n",
        "\n",
        "# ğŸ”· ê¸°ì¡´ Reciprocity ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ ì§‘í•© ê°€ì ¸ì˜¤ê¸°\n",
        "base_words = set(lexicon.cats['help']) \\\n",
        "           | set(lexicon.cats['politeness']) \\\n",
        "           | set(lexicon.cats['sympathy'])\n",
        "\n",
        "print(f\"âœ… ê¸°ì¡´ Reciprocity ë‹¨ì–´ ìˆ˜: {len(base_words)}\")\n",
        "\n",
        "# ğŸ”· ìµœì¢… í™•ì¥ëœ ì‚¬ìš©ì ì •ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "reddit_reciprocity_words_final = [\n",
        "    # â€¦ (ë„ˆê°€ ì‘ì„±í•œ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ)\n",
        "]\n",
        "\n",
        "# ğŸ”· ì¤‘ë³µ ì œê±° & ì •ë ¬\n",
        "reddit_reciprocity_words_unique = sorted({\n",
        "    word for word in reddit_reciprocity_words_final if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"âœ… ì¤‘ë³µ ì œê±° í›„ ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ ìˆ˜: {len(reddit_reciprocity_words_unique)}\")\n",
        "print(reddit_reciprocity_words_unique)\n",
        "\n",
        "# ğŸ”· ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ ìƒì„±\n",
        "lexicon.create_category(\"reddit_reciprocity\", reddit_reciprocity_words_unique)\n",
        "print(\"âœ… ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ 'reddit_reciprocity' ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# ğŸ”· ì¹´í…Œê³ ë¦¬ ì •ìƒ ë“±ë¡ í…ŒìŠ¤íŠ¸\n",
        "test_result = lexicon.analyze(\"thank you for your help\", categories=[\n",
        "    \"help\", \"politeness\", \"sympathy\", \"reddit_reciprocity\"\n",
        "])\n",
        "expected_categories = [\"help\", \"politeness\", \"sympathy\", \"reddit_reciprocity\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"âŒ í•˜ë‚˜ ì´ìƒì˜ ì¹´í…Œê³ ë¦¬ê°€ ì˜¬ë°”ë¥´ê²Œ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ğŸ§  ë¶„ì„ í•¨ìˆ˜: ê¸°ì¡´ + ì‚¬ìš©ì ì •ì˜ + í•©ì‚°\n",
        "def analyze_and_combine_reciprocity(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"help\", \"politeness\", \"sympathy\", \"reddit_reciprocity\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('help', 0.0),\n",
        "        result.get('politeness', 0.0),\n",
        "        result.get('sympathy', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_reciprocity', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"reciprocity_base_score\": round(base_score, 4),\n",
        "        \"reddit_reciprocity_score\": round(reddit_score, 4),\n",
        "        \"combined_reciprocity_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# ğŸƒ ë¶„ì„ ì‹¤í–‰\n",
        "tqdm.pandas()\n",
        "reciprocity_df = df['comment'].progress_apply(analyze_and_combine_reciprocity)\n",
        "\n",
        "# ğŸ”— ë°ì´í„° í•©ì¹˜ê¸°\n",
        "result_df = pd.concat([df, reciprocity_df], axis=1)\n",
        "\n",
        "# ğŸ’¾ ê²°ê³¼ ì €ì¥\n",
        "output_file = filename.replace(\".xlsx\", \"_reciprocity_combined_empath_final.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"âœ… ì™„ë£Œ: ìµœì¢… í™•ì¥ ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ë¥¼ ë°˜ì˜í•œ Reciprocity ì ìˆ˜ê°€ ê³„ì‚°ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "ptfZ1qciBbsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "relational bonding"
      ],
      "metadata": {
        "id": "ohJI4hyEBdXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# â¬†ï¸ ë°ì´í„° ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "print(f\"âœ… ì´ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ\")\n",
        "\n",
        "# ğŸ¤– Empath ë¡œë“œ\n",
        "lexicon = Empath()\n",
        "\n",
        "# ğŸ”· ê¸°ì¡´ Relational Bonding ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ ì§‘í•© ê°€ì ¸ì˜¤ê¸°\n",
        "base_words = set(lexicon.cats['leader']) | set(lexicon.cats['family'])\n",
        "print(f\"âœ… ê¸°ì¡´ Relational Bonding ë‹¨ì–´ ìˆ˜: {len(base_words)}\")\n",
        "\n",
        "# ğŸ”· í™•ì¥ëœ ì‚¬ìš©ì ì •ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "reddit_relational_bonding_words_final = [\n",
        "    # â€¦ (ë„ˆê°€ ì‘ì„±í•œ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ)\n",
        "]\n",
        "\n",
        "# ğŸ”· ì¤‘ë³µ ì œê±° & ì •ë ¬\n",
        "reddit_relational_bonding_words_unique = sorted({\n",
        "    word for word in reddit_relational_bonding_words_final if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"âœ… ì¤‘ë³µ ì œê±° í›„ ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ ìˆ˜: {len(reddit_relational_bonding_words_unique)}\")\n",
        "print(reddit_relational_bonding_words_unique)\n",
        "\n",
        "# ğŸ”· ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ ìƒì„±\n",
        "lexicon.create_category(\"reddit_relational_bonding\", reddit_relational_bonding_words_unique)\n",
        "print(\"âœ… ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ 'reddit_relational_bonding' ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# ğŸ”· ì¹´í…Œê³ ë¦¬ ì •ìƒ ë“±ë¡ í…ŒìŠ¤íŠ¸\n",
        "test_result = lexicon.analyze(\"we are a team and I appreciate you\", categories=[\n",
        "    \"leader\", \"family\", \"reddit_relational_bonding\"\n",
        "])\n",
        "expected_categories = [\"leader\", \"family\", \"reddit_relational_bonding\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"âŒ í•˜ë‚˜ ì´ìƒì˜ ì¹´í…Œê³ ë¦¬ê°€ ì˜¬ë°”ë¥´ê²Œ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ğŸ§  ë¶„ì„ í•¨ìˆ˜: ê¸°ì¡´ + ì‚¬ìš©ì ì •ì˜ + í•©ì‚°\n",
        "def analyze_and_combine_relational_bonding(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"leader\", \"family\", \"reddit_relational_bonding\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('leader', 0.0),\n",
        "        result.get('family', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_relational_bonding', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"relational_bonding_base_score\": round(base_score, 4),\n",
        "        \"reddit_relational_bonding_score\": round(reddit_score, 4),\n",
        "        \"combined_relational_bonding_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# ğŸƒ ë¶„ì„ ì‹¤í–‰\n",
        "tqdm.pandas()\n",
        "relational_df = df['comment'].progress_apply(analyze_and_combine_relational_bonding)\n",
        "\n",
        "# ğŸ”— ë°ì´í„° í•©ì¹˜ê¸°\n",
        "result_df = pd.concat([df, relational_df], axis=1)\n",
        "\n",
        "# ğŸ’¾ ê²°ê³¼ ì €ì¥\n",
        "output_file = filename.replace(\".xlsx\", \"_relational_bonding_combined_empath_final.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"âœ… ì™„ë£Œ: ìµœì¢… í™•ì¥ ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ë¥¼ ë°˜ì˜í•œ Relational Bonding ì ìˆ˜ê°€ ê³„ì‚°ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "Vx3Tg890Bfiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "affective connectedness"
      ],
      "metadata": {
        "id": "9eRTZPcGBiC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# â¬†ï¸ ë°ì´í„° ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# âœ… í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
        "\n",
        "print(f\"âœ… ì´ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ\")\n",
        "\n",
        "# ğŸ¤– Empath ë¡œë“œ\n",
        "lexicon = Empath()\n",
        "\n",
        "# ğŸ”· ê¸°ì¡´ Affective Connectedness ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ ì§‘í•© ê°€ì ¸ì˜¤ê¸°\n",
        "base_words = set(lexicon.cats['emotional']) \\\n",
        "           | set(lexicon.cats['affection']) \\\n",
        "           | set(lexicon.cats['joy']) \\\n",
        "           | set(lexicon.cats['sadness']) \\\n",
        "           | set(lexicon.cats['suffering'])\n",
        "\n",
        "print(f\"âœ… ê¸°ì¡´ Affective Connectedness ë‹¨ì–´ ìˆ˜: {len(base_words)}\")\n",
        "\n",
        "# ğŸ”· Reddit íŠ¹í™” Affective Connectedness ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸\n",
        "reddit_affective_connectedness_words = [\n",
        "    # â€¦ (ë„ˆê°€ ì‘ì„±í•œ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ)\n",
        "]\n",
        "\n",
        "# ğŸ”· ì¤‘ë³µ ì œê±° & ì •ë ¬\n",
        "reddit_affective_connectedness_words_unique = sorted({\n",
        "    word for word in reddit_affective_connectedness_words if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"âœ… ì¤‘ë³µ ì œê±° í›„ ì‚¬ìš©ì ì •ì˜ ë‹¨ì–´ ìˆ˜: {len(reddit_affective_connectedness_words_unique)}\")\n",
        "print(reddit_affective_connectedness_words_unique)\n",
        "\n",
        "# ğŸ”· ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ ìƒì„±\n",
        "lexicon.create_category(\"reddit_affective_connectedness\", reddit_affective_connectedness_words_unique)\n",
        "print(\"âœ… ì‚¬ìš©ì ì •ì˜ ì¹´í…Œê³ ë¦¬ 'reddit_affective_connectedness' ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "# ğŸ”· ì¹´í…Œê³ ë¦¬ ì •ìƒ ë“±ë¡ í…ŒìŠ¤íŠ¸\n",
        "test_result = lexicon.analyze(\"sending love and warm hugs\", categories=[\n",
        "    \"emotional\", \"affection\", \"joy\", \"sadness\", \"suffering\", \"reddit_affective_connectedness\"\n",
        "])\n",
        "expected_categories = [\"emotional\", \"affection\", \"joy\", \"sadness\", \"suffering\", \"reddit_affective_connectedness\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"âŒ í•˜ë‚˜ ì´ìƒì˜ ì¹´í…Œê³ ë¦¬ê°€ ì˜¬ë°”ë¥´ê²Œ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ğŸ§  ë¶„ì„ í•¨ìˆ˜: ê¸°ì¡´ + ì‚¬ìš©ì ì •ì˜ + í•©ì‚°\n",
        "def analyze_and_combine_affective_connectedness(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"emotional\", \"affection\", \"joy\", \"sadness\", \"suffering\", \"reddit_affective_connectedness\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('emotional', 0.0),\n",
        "        result.get('affection', 0.0),\n",
        "        result.get('joy', 0.0),\n",
        "        result.get('sadness', 0.0),\n",
        "        result.get('suffering', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_affective_connectedness', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"affective_connectedness_base_score\": round(base_score, 4),\n",
        "        \"reddit_affective_connectedness_score\": round(reddit_score, 4),\n",
        "        \"combined_affective_connectedness_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# ğŸƒ ë¶„ì„ ì‹¤í–‰\n",
        "tqdm.pandas()\n",
        "affective_df = df['comment'].progress_apply(analyze_and_combine_affective_connectedness)\n",
        "\n",
        "# ğŸ”— ë°ì´í„° í•©ì¹˜ê¸°\n",
        "result_df = pd.concat([df, affective_df], axis=1)\n",
        "\n",
        "# ğŸ’¾ ê²°ê³¼ ì €ì¥\n",
        "output_file = filename.replace(\".xlsx\", \"_affective_connectedness_combined_empath.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"âœ… ì™„ë£Œ: ê¸°ì¡´ + ì‚¬ìš©ì ì •ì˜ Affective Connectedness ì ìˆ˜ê°€ ê³„ì‚°ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "-Gko4mS_BjUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Mediator (self-disclosure)"
      ],
      "metadata": {
        "id": "C5KqhHiHCt5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. í™˜ê²½ ì„¤ì •\n",
        "!pip install pandas openpyxl nltk\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # Colab ë©”ëª¨ë¦¬ ì´ˆê³¼ ë°©ì§€\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import files\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 2. íŒŒì¼ ì—…ë¡œë“œ\n",
        "print(\"ğŸ‘‰ ëŒ“ê¸€ ë°ì´í„° (.xlsx) ì—…ë¡œë“œí•˜ì„¸ìš”\")\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(file_path)\n",
        "df['comment'] = df['comment'].astype(str)\n",
        "\n",
        "# 3. ë‹¨ì–´ ìˆ˜ í•„í„°ë§\n",
        "df['word_count'] = df['comment'].apply(lambda x: len(re.findall(r'\\b\\w+\\b', x.lower())))\n",
        "df = df[df['word_count'] >= 10].copy()\n",
        "\n",
        "# 4. LIWC-lite ì‚¬ì „ ì •ì˜\n",
        "liwc_dict = {\n",
        "    \"i\": [\"i\"], \"me\": [\"i\"], \"my\": [\"i\"], \"mine\": [\"i\"], \"myself\": [\"i\"],\n",
        "    \"i'm\": [\"i\"], \"iâ€™ll\": [\"i\"], \"iâ€™ve\": [\"i\"], \"iâ€™d\": [\"i\"],\n",
        "    \"feel\": [\"affect\"], \"felt\": [\"affect\"], \"emotion\": [\"affect\"], \"emotions\": [\"affect\"],\n",
        "    \"emotional\": [\"affect\"], \"feelings\": [\"affect\"],\n",
        "    \"love\": [\"affect\", \"posemo\"], \"like\": [\"affect\", \"posemo\"], \"good\": [\"affect\", \"posemo\"],\n",
        "    \"great\": [\"affect\", \"posemo\"], \"awesome\": [\"affect\", \"posemo\"], \"amazing\": [\"affect\", \"posemo\"],\n",
        "    \"fun\": [\"affect\", \"posemo\"], \"better\": [\"affect\", \"posemo\"], \"enjoy\": [\"affect\", \"posemo\"],\n",
        "    \"happy\": [\"affect\", \"posemo\"],\n",
        "    \"sad\": [\"affect\", \"negemo\"], \"angry\": [\"affect\", \"negemo\"], \"lonely\": [\"affect\", \"negemo\"],\n",
        "    \"anxious\": [\"affect\", \"negemo\"], \"scared\": [\"affect\", \"negemo\"], \"cry\": [\"affect\", \"negemo\"],\n",
        "    \"abuse\": [\"affect\", \"negemo\"], \"paranoid\": [\"affect\", \"negemo\"], \"worry\": [\"affect\", \"negemo\"],\n",
        "    \"depressed\": [\"affect\", \"negemo\"],\n",
        "    \"think\": [\"cogproc\"], \"know\": [\"cogproc\"], \"understand\": [\"cogproc\"],\n",
        "    \"guess\": [\"cogproc\"], \"realize\": [\"cogproc\"], \"believe\": [\"cogproc\"],\n",
        "    \"consider\": [\"cogproc\"], \"aware\": [\"cogproc\"], \"want\": [\"cogproc\"],\n",
        "    \"make\": [\"cogproc\"], \"need\": [\"cogproc\"], \"real\": [\"cogproc\"],\n",
        "    \"talk\": [\"social\"], \"talking\": [\"social\"], \"friend\": [\"social\"],\n",
        "    \"relationship\": [\"social\"], \"partner\": [\"social\"], \"chat\": [\"social\"],\n",
        "    \"connect\": [\"social\"], \"people\": [\"social\"], \"app\": [\"social\"],\n",
        "    \"replika\": [\"social\"],\n",
        "    \"tired\": [\"bio\"], \"sick\": [\"bio\"], \"pain\": [\"bio\"], \"health\": [\"bio\"],\n",
        "    \"sleep\": [\"bio\"], \"energy\": [\"bio\"], \"hungry\": [\"bio\"], \"mental\": [\"bio\"]\n",
        "}\n",
        "liwc_categories = sorted(set(cat for cats in liwc_dict.values() for cat in cats))\n",
        "\n",
        "# 5. ì ìˆ˜ ê³„ì‚°\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def analyze_liwc_lemma(text):\n",
        "    tokens = re.sub(r\"[^\\w\\s]\", \"\", text.lower()).split()\n",
        "    lemmas = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
        "    counts = dict.fromkeys(liwc_categories, 0)\n",
        "    total = len(lemmas)\n",
        "    for tok in lemmas:\n",
        "        if tok in liwc_dict:\n",
        "            for cat in liwc_dict[tok]:\n",
        "                counts[cat] += 1\n",
        "    return [counts[cat] / total if total else 0.0 for cat in liwc_categories]\n",
        "\n",
        "liwc_vectors = df['comment'].apply(analyze_liwc_lemma).tolist()\n",
        "X = np.array(liwc_vectors)\n",
        "\n",
        "target_cats = ['i', 'affect', 'negemo', 'cogproc', 'bio', 'social']\n",
        "target_indices = [liwc_categories.index(cat) for cat in target_cats]\n",
        "df['disclosure_score'] = X[:, target_indices].mean(axis=1)\n",
        "\n",
        "# 6. ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
        "output_file = \"replika_disclosure_score_filtered.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "KrFtoJQrC3-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Moderator (subjectivity)"
      ],
      "metadata": {
        "id": "UJ1aUXJ6C56m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "# âœ… GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ ë° ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"ğŸš€ Using device:\", \"GPU\" if device == 0 else \"CPU\")\n",
        "\n",
        "# â¬†ï¸ ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ ì—‘ì…€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# âœ… 'comment' ì—´ ì¡´ì¬ í™•ì¸\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ì—‘ì…€ì— ì—†ìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì—´ ì´ë¦„ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”!\")\n",
        "\n",
        "# ğŸ¤– Zero-shot ë¶„ë¥˜ê¸° ë¡œë“œ (GPU ì§€ì •)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
        "\n",
        "# ğŸ¯ í‰ê°€ í•­ëª© ë¼ë²¨ (Subjectivityë§Œ)\n",
        "labels = [\"Subjectivity\"]\n",
        "\n",
        "# ğŸ§© Subjectivity ì ìˆ˜ ì¶”ì¶œ í•¨ìˆ˜\n",
        "def classify_Subjectivity(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "    try:\n",
        "        result = classifier(comment, candidate_labels=labels)\n",
        "        score = round(result[\"scores\"][0], 4)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "# ğŸƒ ë¶„ì„ ì‹¤í–‰ (ì§„í–‰ë¥  í‘œì‹œ)\n",
        "tqdm.pandas()\n",
        "df['Subjectivity_score'] = df['comment'].progress_apply(classify_Subjectivity)\n",
        "\n",
        "# ğŸ’¾ ê²°ê³¼ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
        "output_filename = filename.replace(\".xlsx\", \"_Subjectivity_score.xlsx\")\n",
        "df.to_excel(output_filename, index=False)\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "76GQmNDrDdrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dependent Variable (sentiment)"
      ],
      "metadata": {
        "id": "XNIxKPqZDhSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install scipy transformers pandas openpyxl tqdm matplotlib datasets scikit-learn --quiet"
      ],
      "metadata": {
        "id": "6-GaQbINDlcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì • (GPU ì‚¬ìš©)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# ğŸ“„ ë°ì´í„° ë¡œë”©\n",
        "df = pd.read_excel(filename)\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"âŒ 'comment' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "df['Review'] = df['comment'].fillna(\"\")\n",
        "\n",
        "# ğŸ¤– ëª¨ë¸ ë¡œë”© (ì‚¬ì „ ë¡œë”© ìºì‹œ í™œìš©)\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
        "model.eval()\n",
        "\n",
        "# âœ… ê°ì„± ë¶„ì„ í•¨ìˆ˜ (ìµœì í™”: ê³ ì • ê¸¸ì´, GPU ì§ì ‘ ì…ë ¥)\n",
        "def analyze_sentiment_batch(texts):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",     # ì†ë„ ì¦ê°€\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded)\n",
        "    probs = softmax(output.logits.cpu().numpy(), axis=1)\n",
        "    return probs\n",
        "\n",
        "# âœ… ë°°ì¹˜ ë¶„ì„ (í° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)\n",
        "batch_size = 64   # ë” í¬ë©´ ì†ë„ â†‘ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 32ë¡œ)\n",
        "scores = []\n",
        "\n",
        "for i in tqdm(range(0, len(df), batch_size), desc=\"Analyzing\"):\n",
        "    batch = df['Review'].iloc[i:i+batch_size].tolist()\n",
        "    try:\n",
        "        probs = analyze_sentiment_batch(batch)\n",
        "        for p in probs:\n",
        "            pos, neu, neg = p[2], p[1], p[0]\n",
        "            compound = round((pos - neg) * (1 - neu), 4)\n",
        "            scores.append(compound)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error at batch {i}: {e}\")\n",
        "        scores.extend([0.0] * len(batch))\n",
        "\n",
        "# âœ… ê²°ê³¼ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
        "df['sentiment_score'] = scores\n",
        "output_file = filename.replace(\".xlsx\", \"_roberta_sentiment_score_optimized.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"âœ… ìµœì í™”ëœ RoBERTa ê°ì„± ë¶„ì„ ì™„ë£Œ! ğŸš€\")"
      ],
      "metadata": {
        "id": "aMiLEO2sDm49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}