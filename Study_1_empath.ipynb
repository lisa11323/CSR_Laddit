{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisa11323/CSR_Laddit/blob/main/Study_1_empath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Preprocessed data"
      ],
      "metadata": {
        "id": "gmDuakhNBuh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove non-English characters + strip emojis + filter by syllable count"
      ],
      "metadata": {
        "id": "Toh7dJxLCTJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pandas numpy\n",
        "!pip install nltk emoji"
      ],
      "metadata": {
        "id": "NhkamJh_CH2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# 📁 파일 업로드\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# ⏳ tqdm과 pandas 연동\n",
        "tqdm.pandas()\n",
        "\n",
        "# 📄 엑셀 파일 읽기\n",
        "df = pd.read_excel(file_name)\n",
        "\n",
        "# 🧹 이모지 + 비영어 문자 제거 함수\n",
        "def clean_text(text):\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    text = emoji.replace_emoji(text, replace=\"\")\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# 🔄 전처리 적용 (기존 'Review' 덮어쓰기)\n",
        "df['Review'] = df['Review'].progress_apply(clean_text)\n",
        "\n",
        "# ✨ 리뷰 길이 계산 함수\n",
        "def get_review_length(text):\n",
        "    try:\n",
        "        return len(str(text).split())\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# 📏 Review_length 컬럼 생성\n",
        "df['Review_length'] = df['Review'].progress_apply(get_review_length)\n",
        "\n",
        "# 🚫 리뷰 길이 10 미만 제거, 기준을 바꾸려면 여기에 있는 숫자 10을 수정\n",
        "df = df[df['Review_length'] >= 10].reset_index(drop=True)\n",
        "\n",
        "# ✅ 엑셀로 저장\n",
        "preprocessed_file_path = 'preprocessed_comments_with_length.xlsx'\n",
        "df.to_excel(preprocessed_file_path, index=False)\n",
        "print(f\"✅ 전처리 + 리뷰 길이 ≥10 필터링 데이터를 '{preprocessed_file_path}' 파일로 저장했습니다.\")\n",
        "\n",
        "# 📥 다운로드\n",
        "files.download(preprocessed_file_path)"
      ],
      "metadata": {
        "id": "a9z1PF8JCIYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the Top-N most frequent words from the Reddit comment data"
      ],
      "metadata": {
        "id": "8-pMqIM7CLsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 데이터에 없습니다!\")\n",
        "\n",
        "print(f\"✅ 총 댓글 수: {len(df)}개\")\n",
        "\n",
        "# 모든 댓글을 하나의 문자열로 합치기\n",
        "all_text = ' '.join(df['comment'].dropna().astype(str).tolist()).lower()\n",
        "\n",
        "# 단어만 추출\n",
        "words = re.findall(r'\\b\\w+\\b', all_text)\n",
        "\n",
        "# 단어 빈도 계산\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# 상위 50개 단어 출력\n",
        "top_n = 50\n",
        "print(\"\\n🔷 상위 단어 Top-50:\")\n",
        "for word, count in word_counts.most_common(top_n):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "t8cWv_GsBuKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 제거\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [w for w in words if w not in stop_words]\n",
        "\n",
        "# 불용어 제거 후 단어 빈도 재계산\n",
        "filtered_word_counts = Counter(filtered_words)\n",
        "\n",
        "print(\"\\n🔷 불용어 제거 후 상위 단어 Top-50:\")\n",
        "for word, count in filtered_word_counts.most_common(50):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "gE6WJn5hB-r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Independent Variable"
      ],
      "metadata": {
        "id": "xY9d4X1WAtG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trust"
      ],
      "metadata": {
        "id": "TEt8K9yvA-XP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClxZ5pV4Ab0z"
      },
      "outputs": [],
      "source": [
        "# 📦 Install libraries\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# 📚 Import libraries\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ⬆️ Upload data\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 Load data\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# ✅ Check required column\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ The 'comment' column is missing from the data!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"❌ The 'comment' column contains no data to analyze!\")\n",
        "\n",
        "print(f\"✅ Total number of comments: {len(df)}\")\n",
        "\n",
        "# 🤖 Load Empath\n",
        "lexicon = Empath()\n",
        "print(\"🔷 The default 'trust' category in Empath is handled internally.\")\n",
        "\n",
        "# 🔷 Custom Reddit-specific trust word list\n",
        "reddit_trust_words_full = [\n",
        "    'vouch', 'got', 'back', 'solid', 'faithful', 'reliable', 'loyal',\n",
        "    'stand', 'based', 'legit', 'support', 'believe', 'integrity',\n",
        "    'ride', 'crew', 'dayone', 'fam', 'homie', 'kin',\n",
        "    'respect', 'props', 'cover', 'cosign', 'trustworthy', 'dependable',\n",
        "    'buddy', 'partner', 'ally', 'friend', 'assist', 'helpful'\n",
        "]\n",
        "\n",
        "# ✅ Remove duplicates & sort\n",
        "reddit_trust_words_unique = sorted(set(reddit_trust_words_full))\n",
        "print(f\"🔷 Number of unique Reddit-specific trust words: {len(reddit_trust_words_unique)}\")\n",
        "print(reddit_trust_words_unique)\n",
        "\n",
        "# 🔷 Create custom category\n",
        "lexicon.create_category(\"reddit_trust\", reddit_trust_words_unique)\n",
        "print(\"✅ Custom category 'reddit_trust' created successfully!\")\n",
        "\n",
        "# 🔷 Verify that categories are properly registered (optional)\n",
        "test_result = lexicon.analyze(\"this is a test\", categories=[\"trust\", \"reddit_trust\"])\n",
        "if not all(cat in test_result for cat in [\"trust\", \"reddit_trust\"]):\n",
        "    raise RuntimeError(\"❌ Either 'trust' or 'reddit_trust' category was not registered correctly!\")\n",
        "\n",
        "# 🧠 Analysis function: default trust + custom reddit_trust + combined\n",
        "def analyze_and_combine_trust(text):\n",
        "    result = lexicon.analyze(str(text), categories=[\"trust\", \"reddit_trust\"], normalize=True)\n",
        "    trust_score = result.get('trust', 0.0)\n",
        "    reddit_trust_score = result.get('reddit_trust', 0.0)\n",
        "    combined_trust = trust_score + reddit_trust_score\n",
        "    return pd.Series({\n",
        "        \"trust_score\": round(trust_score, 4),\n",
        "        \"reddit_trust_score\": round(reddit_trust_score, 4),\n",
        "        \"combined_trust_score\": round(combined_trust, 4)\n",
        "    })\n",
        "\n",
        "# 🏃 Run analysis\n",
        "tqdm.pandas()\n",
        "trust_df = df['comment'].progress_apply(analyze_and_combine_trust)\n",
        "\n",
        "# 🔗 Combine results\n",
        "result_df = pd.concat([df, trust_df], axis=1)\n",
        "\n",
        "# 💾 Save results\n",
        "output_file = filename.replace(\".xlsx\", \"_trust_combined_empath_expanded.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"✅ Done: Metrics including the extended custom trust scores have been calculated and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "social engagement"
      ],
      "metadata": {
        "id": "VXIDm3IwBUyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 라이브러리 설치\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# 📚 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ⬆️ 데이터 업로드\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 데이터 불러오기\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# ✅ 필수 컬럼 체크\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 데이터에 없습니다!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"❌ 'comment' 열에 분석할 데이터가 없습니다!\")\n",
        "\n",
        "print(f\"✅ 총 댓글 수: {len(df)}개\")\n",
        "\n",
        "# 🤖 Empath 로드\n",
        "lexicon = Empath()\n",
        "\n",
        "# 🔷 기존 Social Engagement 카테고리 단어 집합 가져오기\n",
        "base_words = set(lexicon.cats['communication']) \\\n",
        "           | set(lexicon.cats['meeting']) \\\n",
        "           | set(lexicon.cats['social_media'])\n",
        "\n",
        "print(f\"✅ 기존 Social Engagement 단어 수: {len(base_words)}\")\n",
        "\n",
        "# 🔷 Reddit 특화 Social Engagement 단어 리스\n",
        "reddit_social_engagement_words_full = [\n",
        "    'subreddit', 'thread', 'reply', 'post', 'topic', 'comment', 'dm',\n",
        "    'debate', 'discussion', 'poll', 'vote', 'feedback', 'engage',\n",
        "    'interact', 'participate',\n",
        "    'connect', 'bond', 'exchange', 'help', 'support',\n",
        "    'mention', 'tag', 'like', 'share', 'follow', 'trend'\n",
        "]\n",
        "\n",
        "# 🔷 중복 제거 & 정렬\n",
        "reddit_social_engagement_words_unique = sorted({\n",
        "    word for word in reddit_social_engagement_words_full if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"✅ 중복 제거 후 사용자 정의 단어 수: {len(reddit_social_engagement_words_unique)}\")\n",
        "print(reddit_social_engagement_words_unique)\n",
        "\n",
        "# 🔷 사용자 정의 카테고리 생성\n",
        "lexicon.create_category(\"reddit_social_engagement\", reddit_social_engagement_words_unique)\n",
        "print(\"✅ 사용자 정의 카테고리 'reddit_social_engagement' 생성 완료!\")\n",
        "\n",
        "# 🔷 카테고리 정상 등록 테스트\n",
        "test_result = lexicon.analyze(\"this is a test post reply engage\", categories=[\n",
        "    \"communication\", \"meeting\", \"social_media\", \"reddit_social_engagement\"\n",
        "])\n",
        "expected_categories = [\"communication\", \"meeting\", \"social_media\", \"reddit_social_engagement\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"❌ 하나 이상의 카테고리가 올바르게 등록되지 않았습니다.\")\n",
        "\n",
        "# 🧠 분석 함수: 기존 + 사용자 정의 + 합산\n",
        "def analyze_and_combine_social_engagement(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"communication\", \"meeting\", \"social_media\", \"reddit_social_engagement\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('communication', 0.0),\n",
        "        result.get('meeting', 0.0),\n",
        "        result.get('social_media', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_social_engagement', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"social_engagement_base_score\": round(base_score, 4),\n",
        "        \"reddit_social_engagement_score\": round(reddit_score, 4),\n",
        "        \"combined_social_engagement_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# 🏃 분석 실행\n",
        "tqdm.pandas()\n",
        "engagement_df = df['comment'].progress_apply(analyze_and_combine_social_engagement)\n",
        "\n",
        "# 🔗 데이터 합치기\n",
        "result_df = pd.concat([df, engagement_df], axis=1)\n",
        "\n",
        "# 💾 결과 저장\n",
        "output_file = filename.replace(\".xlsx\", \"_social_engagement_combined_empath.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"✅ 완료: 기존 + 사용자 정의 Social Engagement 점수가 계산되어 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "qCJOr9JNBV9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Norms of reciprocity"
      ],
      "metadata": {
        "id": "m8jeb8M9BYSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 라이브러리 설치\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# 📚 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ⬆️ 데이터 업로드\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 데이터 불러오기\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# ✅ 필수 컬럼 체크\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 데이터에 없습니다!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"❌ 'comment' 열에 분석할 데이터가 없습니다!\")\n",
        "\n",
        "print(f\"✅ 총 댓글 수: {len(df)}개\")\n",
        "\n",
        "# 🤖 Empath 로드\n",
        "lexicon = Empath()\n",
        "\n",
        "# 🔷 기존 Reciprocity 카테고리 단어 집합 가져오기\n",
        "base_words = set(lexicon.cats['help']) \\\n",
        "           | set(lexicon.cats['politeness']) \\\n",
        "           | set(lexicon.cats['sympathy'])\n",
        "\n",
        "print(f\"✅ 기존 Reciprocity 단어 수: {len(base_words)}\")\n",
        "\n",
        "# 🔷 최종 확장된 사용자 정의 리스트\n",
        "reddit_reciprocity_words_final = [\n",
        "    # … (너가 작성한 리스트 그대로)\n",
        "]\n",
        "\n",
        "# 🔷 중복 제거 & 정렬\n",
        "reddit_reciprocity_words_unique = sorted({\n",
        "    word for word in reddit_reciprocity_words_final if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"✅ 중복 제거 후 사용자 정의 단어 수: {len(reddit_reciprocity_words_unique)}\")\n",
        "print(reddit_reciprocity_words_unique)\n",
        "\n",
        "# 🔷 사용자 정의 카테고리 생성\n",
        "lexicon.create_category(\"reddit_reciprocity\", reddit_reciprocity_words_unique)\n",
        "print(\"✅ 사용자 정의 카테고리 'reddit_reciprocity' 생성 완료!\")\n",
        "\n",
        "# 🔷 카테고리 정상 등록 테스트\n",
        "test_result = lexicon.analyze(\"thank you for your help\", categories=[\n",
        "    \"help\", \"politeness\", \"sympathy\", \"reddit_reciprocity\"\n",
        "])\n",
        "expected_categories = [\"help\", \"politeness\", \"sympathy\", \"reddit_reciprocity\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"❌ 하나 이상의 카테고리가 올바르게 등록되지 않았습니다.\")\n",
        "\n",
        "# 🧠 분석 함수: 기존 + 사용자 정의 + 합산\n",
        "def analyze_and_combine_reciprocity(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"help\", \"politeness\", \"sympathy\", \"reddit_reciprocity\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('help', 0.0),\n",
        "        result.get('politeness', 0.0),\n",
        "        result.get('sympathy', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_reciprocity', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"reciprocity_base_score\": round(base_score, 4),\n",
        "        \"reddit_reciprocity_score\": round(reddit_score, 4),\n",
        "        \"combined_reciprocity_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# 🏃 분석 실행\n",
        "tqdm.pandas()\n",
        "reciprocity_df = df['comment'].progress_apply(analyze_and_combine_reciprocity)\n",
        "\n",
        "# 🔗 데이터 합치기\n",
        "result_df = pd.concat([df, reciprocity_df], axis=1)\n",
        "\n",
        "# 💾 결과 저장\n",
        "output_file = filename.replace(\".xlsx\", \"_reciprocity_combined_empath_final.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"✅ 완료: 최종 확장 사용자 정의 단어를 반영한 Reciprocity 점수가 계산되어 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "ptfZ1qciBbsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "relational bonding"
      ],
      "metadata": {
        "id": "ohJI4hyEBdXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 라이브러리 설치\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# 📚 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ⬆️ 데이터 업로드\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 데이터 불러오기\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# ✅ 필수 컬럼 체크\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 데이터에 없습니다!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"❌ 'comment' 열에 분석할 데이터가 없습니다!\")\n",
        "\n",
        "print(f\"✅ 총 댓글 수: {len(df)}개\")\n",
        "\n",
        "# 🤖 Empath 로드\n",
        "lexicon = Empath()\n",
        "\n",
        "# 🔷 기존 Relational Bonding 카테고리 단어 집합 가져오기\n",
        "base_words = set(lexicon.cats['leader']) | set(lexicon.cats['family'])\n",
        "print(f\"✅ 기존 Relational Bonding 단어 수: {len(base_words)}\")\n",
        "\n",
        "# 🔷 확장된 사용자 정의 리스트\n",
        "reddit_relational_bonding_words_final = [\n",
        "    # … (너가 작성한 리스트 그대로)\n",
        "]\n",
        "\n",
        "# 🔷 중복 제거 & 정렬\n",
        "reddit_relational_bonding_words_unique = sorted({\n",
        "    word for word in reddit_relational_bonding_words_final if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"✅ 중복 제거 후 사용자 정의 단어 수: {len(reddit_relational_bonding_words_unique)}\")\n",
        "print(reddit_relational_bonding_words_unique)\n",
        "\n",
        "# 🔷 사용자 정의 카테고리 생성\n",
        "lexicon.create_category(\"reddit_relational_bonding\", reddit_relational_bonding_words_unique)\n",
        "print(\"✅ 사용자 정의 카테고리 'reddit_relational_bonding' 생성 완료!\")\n",
        "\n",
        "# 🔷 카테고리 정상 등록 테스트\n",
        "test_result = lexicon.analyze(\"we are a team and I appreciate you\", categories=[\n",
        "    \"leader\", \"family\", \"reddit_relational_bonding\"\n",
        "])\n",
        "expected_categories = [\"leader\", \"family\", \"reddit_relational_bonding\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"❌ 하나 이상의 카테고리가 올바르게 등록되지 않았습니다.\")\n",
        "\n",
        "# 🧠 분석 함수: 기존 + 사용자 정의 + 합산\n",
        "def analyze_and_combine_relational_bonding(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"leader\", \"family\", \"reddit_relational_bonding\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('leader', 0.0),\n",
        "        result.get('family', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_relational_bonding', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"relational_bonding_base_score\": round(base_score, 4),\n",
        "        \"reddit_relational_bonding_score\": round(reddit_score, 4),\n",
        "        \"combined_relational_bonding_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# 🏃 분석 실행\n",
        "tqdm.pandas()\n",
        "relational_df = df['comment'].progress_apply(analyze_and_combine_relational_bonding)\n",
        "\n",
        "# 🔗 데이터 합치기\n",
        "result_df = pd.concat([df, relational_df], axis=1)\n",
        "\n",
        "# 💾 결과 저장\n",
        "output_file = filename.replace(\".xlsx\", \"_relational_bonding_combined_empath_final.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"✅ 완료: 최종 확장 사용자 정의 단어를 반영한 Relational Bonding 점수가 계산되어 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "Vx3Tg890Bfiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "affective connectedness"
      ],
      "metadata": {
        "id": "9eRTZPcGBiC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 라이브러리 설치\n",
        "!pip install empath pandas tqdm openpyxl --quiet\n",
        "\n",
        "# 📚 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from empath import Empath\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ⬆️ 데이터 업로드\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 데이터 불러오기\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# ✅ 필수 컬럼 체크\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 데이터에 없습니다!\")\n",
        "if df['comment'].isnull().all():\n",
        "    raise ValueError(\"❌ 'comment' 열에 분석할 데이터가 없습니다!\")\n",
        "\n",
        "print(f\"✅ 총 댓글 수: {len(df)}개\")\n",
        "\n",
        "# 🤖 Empath 로드\n",
        "lexicon = Empath()\n",
        "\n",
        "# 🔷 기존 Affective Connectedness 카테고리 단어 집합 가져오기\n",
        "base_words = set(lexicon.cats['emotional']) \\\n",
        "           | set(lexicon.cats['affection']) \\\n",
        "           | set(lexicon.cats['joy']) \\\n",
        "           | set(lexicon.cats['sadness']) \\\n",
        "           | set(lexicon.cats['suffering'])\n",
        "\n",
        "print(f\"✅ 기존 Affective Connectedness 단어 수: {len(base_words)}\")\n",
        "\n",
        "# 🔷 Reddit 특화 Affective Connectedness 단어 리스트\n",
        "reddit_affective_connectedness_words = [\n",
        "    # … (너가 작성한 리스트 그대로)\n",
        "]\n",
        "\n",
        "# 🔷 중복 제거 & 정렬\n",
        "reddit_affective_connectedness_words_unique = sorted({\n",
        "    word for word in reddit_affective_connectedness_words if word not in base_words\n",
        "})\n",
        "\n",
        "print(f\"✅ 중복 제거 후 사용자 정의 단어 수: {len(reddit_affective_connectedness_words_unique)}\")\n",
        "print(reddit_affective_connectedness_words_unique)\n",
        "\n",
        "# 🔷 사용자 정의 카테고리 생성\n",
        "lexicon.create_category(\"reddit_affective_connectedness\", reddit_affective_connectedness_words_unique)\n",
        "print(\"✅ 사용자 정의 카테고리 'reddit_affective_connectedness' 생성 완료!\")\n",
        "\n",
        "# 🔷 카테고리 정상 등록 테스트\n",
        "test_result = lexicon.analyze(\"sending love and warm hugs\", categories=[\n",
        "    \"emotional\", \"affection\", \"joy\", \"sadness\", \"suffering\", \"reddit_affective_connectedness\"\n",
        "])\n",
        "expected_categories = [\"emotional\", \"affection\", \"joy\", \"sadness\", \"suffering\", \"reddit_affective_connectedness\"]\n",
        "if not all(cat in test_result for cat in expected_categories):\n",
        "    raise RuntimeError(\"❌ 하나 이상의 카테고리가 올바르게 등록되지 않았습니다.\")\n",
        "\n",
        "# 🧠 분석 함수: 기존 + 사용자 정의 + 합산\n",
        "def analyze_and_combine_affective_connectedness(text):\n",
        "    result = lexicon.analyze(\n",
        "        str(text),\n",
        "        categories=[\"emotional\", \"affection\", \"joy\", \"sadness\", \"suffering\", \"reddit_affective_connectedness\"],\n",
        "        normalize=True\n",
        "    )\n",
        "    base_score = sum([\n",
        "        result.get('emotional', 0.0),\n",
        "        result.get('affection', 0.0),\n",
        "        result.get('joy', 0.0),\n",
        "        result.get('sadness', 0.0),\n",
        "        result.get('suffering', 0.0)\n",
        "    ])\n",
        "    reddit_score = result.get('reddit_affective_connectedness', 0.0)\n",
        "    combined_score = base_score + reddit_score\n",
        "    return pd.Series({\n",
        "        \"affective_connectedness_base_score\": round(base_score, 4),\n",
        "        \"reddit_affective_connectedness_score\": round(reddit_score, 4),\n",
        "        \"combined_affective_connectedness_score\": round(combined_score, 4)\n",
        "    })\n",
        "\n",
        "# 🏃 분석 실행\n",
        "tqdm.pandas()\n",
        "affective_df = df['comment'].progress_apply(analyze_and_combine_affective_connectedness)\n",
        "\n",
        "# 🔗 데이터 합치기\n",
        "result_df = pd.concat([df, affective_df], axis=1)\n",
        "\n",
        "# 💾 결과 저장\n",
        "output_file = filename.replace(\".xlsx\", \"_affective_connectedness_combined_empath.xlsx\")\n",
        "result_df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"✅ 완료: 기존 + 사용자 정의 Affective Connectedness 점수가 계산되어 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "-Gko4mS_BjUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Mediator (self-disclosure)"
      ],
      "metadata": {
        "id": "C5KqhHiHCt5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 환경 설정\n",
        "!pip install pandas openpyxl nltk\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"  # Colab 메모리 초과 방지\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import files\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 2. 파일 업로드\n",
        "print(\"👉 댓글 데이터 (.xlsx) 업로드하세요\")\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(file_path)\n",
        "df['comment'] = df['comment'].astype(str)\n",
        "\n",
        "# 3. 단어 수 필터링\n",
        "df['word_count'] = df['comment'].apply(lambda x: len(re.findall(r'\\b\\w+\\b', x.lower())))\n",
        "df = df[df['word_count'] >= 10].copy()\n",
        "\n",
        "# 4. LIWC-lite 사전 정의\n",
        "liwc_dict = {\n",
        "    \"i\": [\"i\"], \"me\": [\"i\"], \"my\": [\"i\"], \"mine\": [\"i\"], \"myself\": [\"i\"],\n",
        "    \"i'm\": [\"i\"], \"i’ll\": [\"i\"], \"i’ve\": [\"i\"], \"i’d\": [\"i\"],\n",
        "    \"feel\": [\"affect\"], \"felt\": [\"affect\"], \"emotion\": [\"affect\"], \"emotions\": [\"affect\"],\n",
        "    \"emotional\": [\"affect\"], \"feelings\": [\"affect\"],\n",
        "    \"love\": [\"affect\", \"posemo\"], \"like\": [\"affect\", \"posemo\"], \"good\": [\"affect\", \"posemo\"],\n",
        "    \"great\": [\"affect\", \"posemo\"], \"awesome\": [\"affect\", \"posemo\"], \"amazing\": [\"affect\", \"posemo\"],\n",
        "    \"fun\": [\"affect\", \"posemo\"], \"better\": [\"affect\", \"posemo\"], \"enjoy\": [\"affect\", \"posemo\"],\n",
        "    \"happy\": [\"affect\", \"posemo\"],\n",
        "    \"sad\": [\"affect\", \"negemo\"], \"angry\": [\"affect\", \"negemo\"], \"lonely\": [\"affect\", \"negemo\"],\n",
        "    \"anxious\": [\"affect\", \"negemo\"], \"scared\": [\"affect\", \"negemo\"], \"cry\": [\"affect\", \"negemo\"],\n",
        "    \"abuse\": [\"affect\", \"negemo\"], \"paranoid\": [\"affect\", \"negemo\"], \"worry\": [\"affect\", \"negemo\"],\n",
        "    \"depressed\": [\"affect\", \"negemo\"],\n",
        "    \"think\": [\"cogproc\"], \"know\": [\"cogproc\"], \"understand\": [\"cogproc\"],\n",
        "    \"guess\": [\"cogproc\"], \"realize\": [\"cogproc\"], \"believe\": [\"cogproc\"],\n",
        "    \"consider\": [\"cogproc\"], \"aware\": [\"cogproc\"], \"want\": [\"cogproc\"],\n",
        "    \"make\": [\"cogproc\"], \"need\": [\"cogproc\"], \"real\": [\"cogproc\"],\n",
        "    \"talk\": [\"social\"], \"talking\": [\"social\"], \"friend\": [\"social\"],\n",
        "    \"relationship\": [\"social\"], \"partner\": [\"social\"], \"chat\": [\"social\"],\n",
        "    \"connect\": [\"social\"], \"people\": [\"social\"], \"app\": [\"social\"],\n",
        "    \"replika\": [\"social\"],\n",
        "    \"tired\": [\"bio\"], \"sick\": [\"bio\"], \"pain\": [\"bio\"], \"health\": [\"bio\"],\n",
        "    \"sleep\": [\"bio\"], \"energy\": [\"bio\"], \"hungry\": [\"bio\"], \"mental\": [\"bio\"]\n",
        "}\n",
        "liwc_categories = sorted(set(cat for cats in liwc_dict.values() for cat in cats))\n",
        "\n",
        "# 5. 점수 계산\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def analyze_liwc_lemma(text):\n",
        "    tokens = re.sub(r\"[^\\w\\s]\", \"\", text.lower()).split()\n",
        "    lemmas = [lemmatizer.lemmatize(tok) for tok in tokens]\n",
        "    counts = dict.fromkeys(liwc_categories, 0)\n",
        "    total = len(lemmas)\n",
        "    for tok in lemmas:\n",
        "        if tok in liwc_dict:\n",
        "            for cat in liwc_dict[tok]:\n",
        "                counts[cat] += 1\n",
        "    return [counts[cat] / total if total else 0.0 for cat in liwc_categories]\n",
        "\n",
        "liwc_vectors = df['comment'].apply(analyze_liwc_lemma).tolist()\n",
        "X = np.array(liwc_vectors)\n",
        "\n",
        "target_cats = ['i', 'affect', 'negemo', 'cogproc', 'bio', 'social']\n",
        "target_indices = [liwc_categories.index(cat) for cat in target_cats]\n",
        "df['disclosure_score'] = X[:, target_indices].mean(axis=1)\n",
        "\n",
        "# 6. 저장 및 다운로드\n",
        "output_file = \"replika_disclosure_score_filtered.xlsx\"\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "KrFtoJQrC3-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Moderator (subjectivity)"
      ],
      "metadata": {
        "id": "UJ1aUXJ6C56m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📚 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "# ✅ GPU 사용 여부 확인 및 디바이스 설정\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"🚀 Using device:\", \"GPU\" if device == 0 else \"CPU\")\n",
        "\n",
        "# ⬆️ 엑셀 파일 업로드\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 엑셀 데이터 불러오기\n",
        "df = pd.read_excel(filename)\n",
        "\n",
        "# ✅ 'comment' 열 존재 확인\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 엑셀에 없습니다. 올바른 열 이름인지 확인해주세요!\")\n",
        "\n",
        "# 🤖 Zero-shot 분류기 로드 (GPU 지정)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
        "\n",
        "# 🎯 평가 항목 라벨 (Subjectivity만)\n",
        "labels = [\"Subjectivity\"]\n",
        "\n",
        "# 🧩 Subjectivity 점수 추출 함수\n",
        "def classify_Subjectivity(comment):\n",
        "    if pd.isna(comment) or not str(comment).strip():\n",
        "        return 0.0\n",
        "    try:\n",
        "        result = classifier(comment, candidate_labels=labels)\n",
        "        score = round(result[\"scores\"][0], 4)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        return 0.0\n",
        "\n",
        "# 🏃 분석 실행 (진행률 표시)\n",
        "tqdm.pandas()\n",
        "df['Subjectivity_score'] = df['comment'].progress_apply(classify_Subjectivity)\n",
        "\n",
        "# 💾 결과 저장 및 다운로드\n",
        "output_filename = filename.replace(\".xlsx\", \"_Subjectivity_score.xlsx\")\n",
        "df.to_excel(output_filename, index=False)\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "76GQmNDrDdrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dependent Variable (sentiment)"
      ],
      "metadata": {
        "id": "XNIxKPqZDhSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 라이브러리 설치\n",
        "!pip install scipy transformers pandas openpyxl tqdm matplotlib datasets scikit-learn --quiet"
      ],
      "metadata": {
        "id": "6-GaQbINDlcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📚 라이브러리\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# ✅ 디바이스 설정 (GPU 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 📂 파일 업로드\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# 📄 데이터 로딩\n",
        "df = pd.read_excel(filename)\n",
        "if 'comment' not in df.columns:\n",
        "    raise ValueError(\"❌ 'comment' 열이 없습니다.\")\n",
        "df['Review'] = df['comment'].fillna(\"\")\n",
        "\n",
        "# 🤖 모델 로딩 (사전 로딩 캐시 활용)\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)\n",
        "model.eval()\n",
        "\n",
        "# ✅ 감성 분석 함수 (최적화: 고정 길이, GPU 직접 입력)\n",
        "def analyze_sentiment_batch(texts):\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",     # 속도 증가\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "    )\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded)\n",
        "    probs = softmax(output.logits.cpu().numpy(), axis=1)\n",
        "    return probs\n",
        "\n",
        "# ✅ 배치 분석 (큰 배치 처리로 속도 향상)\n",
        "batch_size = 64   # 더 크면 속도 ↑ (메모리 부족 시 32로)\n",
        "scores = []\n",
        "\n",
        "for i in tqdm(range(0, len(df), batch_size), desc=\"Analyzing\"):\n",
        "    batch = df['Review'].iloc[i:i+batch_size].tolist()\n",
        "    try:\n",
        "        probs = analyze_sentiment_batch(batch)\n",
        "        for p in probs:\n",
        "            pos, neu, neg = p[2], p[1], p[0]\n",
        "            compound = round((pos - neg) * (1 - neu), 4)\n",
        "            scores.append(compound)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error at batch {i}: {e}\")\n",
        "        scores.extend([0.0] * len(batch))\n",
        "\n",
        "# ✅ 결과 저장 및 다운로드\n",
        "df['sentiment_score'] = scores\n",
        "output_file = filename.replace(\".xlsx\", \"_roberta_sentiment_score_optimized.xlsx\")\n",
        "df.to_excel(output_file, index=False)\n",
        "files.download(output_file)\n",
        "\n",
        "print(\"✅ 최적화된 RoBERTa 감성 분석 완료! 🚀\")"
      ],
      "metadata": {
        "id": "aMiLEO2sDm49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}